@misc{1komma5About2025,
  title = {About Us -- {{1KOMMA5}}{$^\circ$}},
  author = {{1KOMMA5{$^\circ$}}},
  year = 2025,
  url = {https://www.1komma5grad.com}
}

@misc{agarwal2025reviewlargelanguagemodels,
  title = {A Review on Large Language Models for Visual Analytics},
  author = {Agarwal, Navya Sonal and Sonbhadra, Sanjay Kumar},
  year = 2025,
  eprint = {2503.15176},
  primaryclass = {cs.HC},
  url = {https://arxiv.org/abs/2503.15176},
  archiveprefix = {arXiv}
}

@misc{amer-yahiaReliableConversationalData2025,
  title = {Towards {{Reliable Conversational Data Analytics}}},
  author = {{Amer-Yahia}, Sihem and Bogojeska, Jasmina and Facchinetti, Roberta and Franceschi, Valeria and Gionis, Aristides and Hose, Katja and Koutrika, Georgia and Kouyos, Roger and Lissandrini, Matteo and Maniu, Silviu and Mirylenka, Katsiaryna and Mottin, Davide and Palpanas, Themis and Rigotti, Mattia and Velegrakis, Yannis},
  year = 2025,
  publisher = {OpenProceedings.org},
  doi = {10.48786/EDBT.2025.78},
  urldate = {2025-11-02},
  abstract = {Conversational AI systems for data analytics aim to enable the extraction of analytical insights by means of conversational interfaces. Such interfaces are powered by a mix of query modalities and machine learning methods for analytics, and are relying on Large Language Models (LLMs) for natural language generation. However, critical challenges hinder their adoption. The question we discuss is how to devise reliable Conversational Data Analytics (CDA) systems producing timely, consistent, and verifiable answers. To reach this goal, we identify five properties that impose a paradigm shift in the way systems are built and in the way they interact with users. To illustrate that shift, we describe a prototypical CDA system. Realizing these properties involves either extending existing components, or redesigning components from scratch; both solutions require overcoming data management challenges and conducting a tight integration with advanced data management and machine learning techniques.},
  langid = {english},
  keywords = {Database Technology},
  file = {/Users/felixriedel/Zotero/storage/8U7KGQKW/Amer-Yahia et al. - 2025 - Towards Reliable Conversational Data Analytics.pdf}
}

@inproceedings{bietal2025,
  title = {{{RBDQ}}: A Reliable {{LLM-based}} Text-to-{{SQL}} System for Business Data Queries},
  booktitle = {Companion Proceedings of the {{ACM}} on Web Conference 2025},
  author = {Bi, Fenglin and Cao, Dongdong and Wang, Zhiyu and Chen, Yang and Zhao, Fangliang and Hu, Tao and Li, Zhi and Zhang, Yanbin and Wang, Wei},
  year = 2025,
  series = {Www '25},
  pages = {95--103},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3701716.3715257},
  abstract = {Using large language models (LLMs) to convert natural language (NL) into SQL simplifies data access for users by allowing them to use everyday language. However, business departments often distrust LLM-based text-to-SQL systems due to the probabilistic nature of SQL generation, which can result in incorrect but executable SQL queries caused by model hallucinations. This leads to significant concerns regarding the accuracy and reliability of the queried data. In this paper, we present RBDQ, a novel LLM-based text-to-SQL system designed to address the unique challenges of business data queries. RBDQ innovatively introduces the Hierarchical Metrics Query Method and integrates advanced Retrieval-Augmented Generation (RAG) methods along with a self-reflection mechanism to tackle these challenges. RBDQ effectively meets the requirements of business metric queries in real-world scenarios. Currently implemented in the Quality Assurance department at ByteDance, RBDQ has significantly improved operational efficiency and query flexibility. Our experiments demonstrate the system's effectiveness, achieving an Execution Accuracy of 96.20\%.},
  isbn = {979-8-4007-1331-6},
  keywords = {business data queries,large language models,retrieval-augmented generation,text-to-sql}
}

@article{chaudhariRLHFDecipheredCritical2025,
  title = {{{RLHF Deciphered}}: {{A Critical Analysis}} of {{Reinforcement Learning}} from {{Human Feedback}} for {{LLMs}}},
  shorttitle = {{{RLHF Deciphered}}},
  author = {Chaudhari, Shreyas and Aggarwal, Pranjal and Murahari, Vishvak and Rajpurohit, Tanmay and Kalyan, Ashwin and Narasimhan, Karthik and Deshpande, Ameet and {Castro da Silva}, Bruno},
  year = 2025,
  month = sep,
  journal = {ACM Comput. Surv.},
  volume = {58},
  number = {2},
  pages = {53:1--53:37},
  issn = {0360-0300},
  doi = {10.1145/3743127},
  urldate = {2025-10-29},
  abstract = {A significant challenge in training large language models (LLMs) as effective assistants is aligning them with human preferences. Reinforcement learning from human feedback (RLHF) has emerged as a promising solution. However, our understanding of RLHF is often limited to initial design choices. This article analyzes RLHF through reinforcement learning principles, focusing on the reward model. It examines modeling choices and function approximation caveats, highlighting assumptions about reward expressivity and revealing limitations like incorrect generalization, model misspecification, and sparse feedback. A categorical review of current literature provides insights for researchers to understand the challenges of RLHF and build upon existing methods.},
  file = {/Users/felixriedel/Zotero/storage/AEJU9Q3E/Chaudhari et al. - 2025 - RLHF Deciphered A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs.pdf}
}

@misc{chhikaraMem0BuildingProductionReady2025,
  title = {Mem0: {{Building Production-Ready AI Agents}} with {{Scalable Long-Term Memory}}},
  shorttitle = {Mem0},
  author = {Chhikara, Prateek and Khant, Dev and Aryan, Saket and Singh, Taranjeet and Yadav, Deshraj},
  year = 2025,
  month = apr,
  number = {arXiv:2504.19413},
  eprint = {2504.19413},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.19413},
  urldate = {2025-10-30},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over prolonged multi-session dialogues. We introduce Mem0, a scalable memory-centric architecture that addresses this issue by dynamically extracting, consolidating, and retrieving salient information from ongoing conversations. Building on this foundation, we further propose an enhanced variant that leverages graph-based memory representations to capture complex relational structures among conversational elements. Through comprehensive evaluations on LOCOMO benchmark, we systematically compare our approaches against six baseline categories: (i) established memory-augmented systems, (ii) retrieval-augmented generation (RAG) with varying chunk sizes and k-values, (iii) a full-context approach that processes the entire conversation history, (iv) an open-source memory solution, (v) a proprietary model system, and (vi) a dedicated memory management platform. Empirical results show that our methods consistently outperform all existing memory systems across four question categories: single-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26\% relative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with graph memory achieves around 2\% higher overall score than the base configuration. Beyond accuracy gains, we also markedly reduce computational overhead compared to full-context method. In particular, Mem0 attains a 91\% lower p95 latency and saves more than 90\% token cost, offering a compelling balance between advanced reasoning capabilities and practical deployment constraints. Our findings highlight critical role of structured, persistent memory mechanisms for long-term conversational coherence, paving the way for more reliable and efficient LLM-driven AI agents.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/felixriedel/Zotero/storage/LHYYBYSP/Chhikara et al. - 2025 - Mem0 Building Production-Ready AI Agents with Scalable Long-Term Memory.pdf}
}

@misc{EURLexOfficialJournal,
  title = {{{EUR-Lex}} - {{Official Journal}} of the {{European Union}}},
  url = {https://eur-lex.europa.eu/TodayOJ/},
  urldate = {2025-10-31}
}

@article{feretzakis2024,
  title = {Trustworthy {{AI}}: {{Securing}} Sensitive Data in Large Language Models},
  author = {Feretzakis, Georgios and Verykios, Vassilios S.},
  year = 2024,
  journal = {AI},
  volume = {5},
  number = {4},
  pages = {2773--2800},
  issn = {2673-2688},
  doi = {10.3390/ai5040134},
  abstract = {Large language models (LLMs) have transformed Natural Language Processing (NLP) by enabling robust text generation and understanding. However, their deployment in sensitive domains like healthcare, finance, and legal services raises critical concerns about privacy and data security. This paper proposes a comprehensive framework for embedding trust mechanisms into LLMs to dynamically control the disclosure of sensitive information. The framework integrates three core components: User Trust Profiling, Information Sensitivity Detection, and Adaptive Output Control. By leveraging techniques such as Role-Based Access Control (RBAC), Attribute-Based Access Control (ABAC), Named Entity Recognition (NER), contextual analysis, and privacy-preserving methods like differential privacy, the system ensures that sensitive information is disclosed appropriately based on the user's trust level. By focusing on balancing data utility and privacy, the proposed solution offers a novel approach to securely deploying LLMs in high-risk environments. Future work will focus on testing this framework across various domains to evaluate its effectiveness in managing sensitive data while maintaining system efficiency.}
}

@article{Gao_2021,
  title = {Advances and Challenges in Conversational Recommender Systems: {{A}} Survey},
  author = {Gao, Chongming and Lei, Wenqiang and He, Xiangnan and {de Rijke}, Maarten and Chua, Tat-Seng},
  year = 2021,
  journal = {AI Open},
  volume = {2},
  pages = {100--126},
  publisher = {Elsevier BV},
  issn = {2666-6510},
  doi = {10.1016/j.aiopen.2021.06.002}
}

@misc{gartner2022conversationalai,
  title = {Gartner Predicts Conversational {{AI}} Will Reduce Contact Center Agent Labor Costs by \$80 Billion in 2026},
  author = {O'Connell, Daniel},
  year = 2022,
  month = aug,
  publisher = {Gartner, Inc.},
  url = {https://www.gartner.com/en/newsroom/press-releases/2022-08-31-gartner-predicts-conversational-ai-will-reduce-contact-center-agent-labor-costs-by-80-billion-in-2026}
}

@misc{GoogleTrends,
  title = {Google {{Trends}}},
  journal = {Google Trends},
  url = {https://trends.google.com/trends/explore?q=AI&date=now%201-d&geo=DE&hl=en},
  urldate = {2025-11-06},
  abstract = {Explore search interest for AI by time, location and popularity on Google Trends},
  langid = {american},
  file = {/Users/felixriedel/Zotero/storage/D6ZE49RK/explore.html}
}

@misc{hou2025modelcontextprotocolmcp,
  title = {Model Context Protocol ({{MCP}}): {{Landscape}}, Security Threats, and Future Research Directions},
  author = {Hou, Xinyi and Zhao, Yanjie and Wang, Shenao and Wang, Haoyu},
  year = 2025,
  eprint = {2503.23278},
  primaryclass = {cs.CR},
  url = {https://arxiv.org/abs/2503.23278},
  archiveprefix = {arXiv}
}

@misc{ibm2024conversational,
  title = {What Is Conversational Analytics?},
  author = {{IBM Corporation}},
  year = 2024,
  url = {https://www.ibm.com/thought-leadership/institute-business-value/en-us/report/conversational-analytics}
}

@inproceedings{kukrejaVectorDatabasesVector2023,
  title = {Vector {{Databases}} and {{Vector Embeddings-Review}}},
  booktitle = {2023 {{International Workshop}} on {{Artificial Intelligence}} and {{Image Processing}} ({{IWAIIP}})},
  author = {Kukreja, Sanjay and Kumar, Tarun and Bharate, Vishal and Purohit, Amit and Dasgupta, Abhijit and Guha, Debashis},
  year = 2023,
  month = dec,
  pages = {231--236},
  doi = {10.1109/IWAIIP58158.2023.10462847},
  urldate = {2025-10-31},
  abstract = {This research paper aims to present a comprehensive survey of vector databases and vector embedding techniques. A concise overview of the evolution, architecture, advantages and challenges of vector databases are presented in this paper. To convert unstructured data into vectors, various embedding techniques with their in-depth technical description are also surveyed in this research paper. The existing vector databases' characteristics and features are described to select an appropriate vector database. The embedding and indexing techniques are thoroughly discussed to help the research community experiment with combining them to develop their application. The challenges of vector databases, like selecting distance metrics, dimensionality, data integrity, cost, etc. are presented. This information on vector database systems will help researchers to exhibit further advancements in their data science applications.},
  keywords = {Artificial Intelligence,Computer Vision,Costs,Data integrity,Data science,Databases,Embeddings,Generative AI,Image processing,Large Language Model,Performance evaluation,Surveys},
  file = {/Users/felixriedel/Zotero/storage/TTGBVQ3N/Kukreja et al. - 2023 - Vector Databases and Vector Embeddings-Review.pdf}
}

@misc{kumaranBuildingText2SQLAgent2025,
  title = {Building a {{Text2SQL Agent With Long-Term Memory}}: {{A Production Grade Architecture}} for {{User Memory}}\dots},
  shorttitle = {Building a {{Text2SQL Agent With Long-Term Memory}}},
  author = {Kumaran, Muthu},
  year = 2025,
  month = may,
  journal = {Data Science Collective},
  url = {https://medium.com/data-science-collective/building-a-text2sql-agent-with-long-term-memory-a-production-grade-architecture-for-user-memory-9d6fc3509e92},
  urldate = {2025-10-30},
  abstract = {How to implement user-specific, long-term memory for smarter database interactions},
  langid = {english},
  file = {/Users/felixriedel/Zotero/storage/PXYBMB5J/MKWriteshere - 2025 - Building a Text2SQL Agent With Long-Term Memory A Production Grade Architecture for User Memoryâ€¦.pdf;/Users/felixriedel/Zotero/storage/LAGGWK9V/building-a-text2sql-agent-with-long-term-memory-a-production-grade-architecture-for-user-memory.html}
}

@article{liuSurveyTexttoSQLEra2025b,
  title = {A {{Survey}} of {{Text-to-SQL}} in the {{Era}} of {{LLMs}}: {{Where Are We}}, and {{Where Are We Going}}?},
  shorttitle = {A {{Survey}} of {{Text-to-SQL}} in the {{Era}} of {{LLMs}}},
  author = {Liu, Xinyu and Shen, Shuyu and Li, Boyan and Ma, Peixian and Jiang, Runzhi and Zhang, Yuxin and Fan, Ju and Li, Guoliang and Tang, Nan and Luo, Yuyu},
  year = 2025,
  month = oct,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {37},
  number = {10},
  pages = {5735--5754},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2025.3592032},
  urldate = {2025-10-29},
  abstract = {Translating users' natural language queries (NL) into SQL queries (i.e., Text-to-SQL, a.k.a. NL2SQL) can significantly reduce barriers to accessing relational databases and support various commercial applications. The performance of Text-to-SQL has been greatly enhanced with the emergence of Large Language Models (LLMs). In this survey, we provide a comprehensive review of Text-to-SQL techniques powered by LLMs, covering its entire lifecycle from the following four aspects: (1) Model: Text-to-SQL translation techniques that tackle not only NL ambiguity and under-specification, but also properly map NL with database schema and instances; (2) Data: From the collection of training data, data synthesis due to training data scarcity, to Text-to-SQL benchmarks; (3) Evaluation: Evaluating Text-to-SQL methods from multiple angles using different metrics and granularities; and (4) Error Analysis: analyzing Text-to-SQL errors to find the root cause and guiding Text-to-SQL models to evolve. Moreover, we offer a rule of thumb for developing Text-to-SQL solutions. Finally, we discuss the research challenges and open problems of Text-to-SQL in the LLMs era.},
  keywords = {Analytical models,Benchmark testing,Data models,database interface,Databases,Error analysis,large language models,Natural language to SQL,Reviews,Structured Query Language,Surveys,Taxonomy,text-to-SQL,Training data},
  file = {/Users/felixriedel/Zotero/storage/NUMLF5B3/Liu et al. - 2025 - A Survey of Text-to-SQL in the Era of LLMs Where Are We, and Where Are We Going.pdf}
}

@misc{majumderCLINContinuallyLearning2023,
  title = {{{CLIN}}: {{A Continually Learning Language Agent}} for {{Rapid Task Adaptation}} and {{Generalization}}},
  shorttitle = {{{CLIN}}},
  author = {Majumder, Bodhisattwa Prasad and Mishra, Bhavana Dalvi and Jansen, Peter and Tafjord, Oyvind and Tandon, Niket and Zhang, Li and {Callison-Burch}, Chris and Clark, Peter},
  year = 2023,
  month = oct,
  number = {arXiv:2310.10134},
  eprint = {2310.10134},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.10134},
  urldate = {2025-10-30},
  abstract = {Language agents have shown some ability to interact with an external environment, e.g., a virtual world such as ScienceWorld, to perform complex tasks, e.g., growing a plant, without the startup costs of reinforcement learning. However, despite their zero-shot capabilities, these agents to date do not continually improve over time, beyond performance refinement on a specific task. Here we present CLIN, the first language-based agent to achieve this, so that it continually improves over multiple trials, including when both the environment and task are varied, and without requiring parameter updates. Our approach is to use a persistent, dynamic, textual memory, centered on causal abstractions (rather than general ``helpful hints''), that is regularly updated after each trial so that the agent gradually learns useful knowledge for new trials. In the ScienceWorld benchmark, CLIN is able to continually improve on repeated trials on the same task and environment, outperforming state-of-the-art reflective language agents like Reflexion by 23 absolute points. CLIN can also transfer its learning to new environments (or new tasks), improving its zero-shot performance by 4 points (13 for new tasks) and can further improve performance there through continual memory updates, enhancing performance by an additional 17 points (7 for new tasks). This suggests a new architecture for agents built on frozen models that can still continually and rapidly improve over time.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/felixriedel/Zotero/storage/WQ97P5RT/Majumder et al. - 2023 - CLIN A Continually Learning Language Agent for Rapid Task Adaptation and Generalization.pdf}
}

@misc{muthukumaranText2SQlAgentwithLongTermMemoryREADMEmdMain,
  title = {{{Text2SQl-Agent-with-Long-Term-Memory}}/{{README}}.Md at Main {$\cdot$} {{MKcodeshere}}/{{Text2SQl-Agent-with-Long-Term-Memory}}},
  author = {{Muthu Kumaran}},
  journal = {GitHub},
  url = {https://github.com/MKcodeshere/Text2SQl-Agent-with-Long-Term-Memory/blob/main/README.md},
  urldate = {2025-10-30},
  abstract = {Contribute to MKcodeshere/Text2SQl-Agent-with-Long-Term-Memory development by creating an account on GitHub.},
  langid = {english},
  file = {/Users/felixriedel/Zotero/storage/HU6EJ8E4/README.html}
}

@article{nejjar2025llms,
  title = {{{LLMs}} for Science: {{Usage}} for Code Generation and Data Analysis},
  author = {Nejjar, Mohamed and Zacharias, Luca and Stiehle, Fabian and Weber, Ingo},
  year = 2025,
  month = jan,
  journal = {Journal of Software: Evolution and Process},
  volume = {37},
  number = {1},
  pages = {e2723},
  doi = {10.1002/smr.2723}
}

@misc{packerMemGPTLLMsOperating2024a,
  title = {{{MemGPT}}: {{Towards LLMs}} as {{Operating Systems}}},
  shorttitle = {{{MemGPT}}},
  author = {Packer, Charles and Wooders, Sarah and Lin, Kevin and Fang, Vivian and Patil, Shishir G. and Stoica, Ion and Gonzalez, Joseph E.},
  year = 2024,
  month = feb,
  number = {arXiv:2310.08560},
  eprint = {2310.08560},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.08560},
  urldate = {2025-10-30},
  abstract = {Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems which provide the illusion of an extended virtual memory via paging between physical memory and disk. Using this technique, we introduce MemGPT (MemoryGPT), a system that intelligently manages different storage tiers in order to effectively provide extended context within the LLM's limited context window. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://research.memgpt.ai.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/felixriedel/Zotero/storage/8U7D6K6B/Packer et al. - 2024 - MemGPT Towards LLMs as Operating Systems.pdf}
}

@article{sunSurveyLargeLanguagea,
  title = {A {{Survey}} on {{Large Language Model-based Agents}} for {{Statistics}} and {{Data Science}}},
  author = {Sun, Maojun and Han, Ruijian and Jiang, Binyan and Qi, Houduo and Sun, Defeng and Yuan, Yancheng and Huang, Jian},
  journal = {The American Statistician},
  volume = {0},
  number = {0},
  pages = {1--14},
  publisher = {ASA Website},
  issn = {0003-1305},
  doi = {10.1080/00031305.2025.2561140},
  urldate = {2025-10-30},
  abstract = {In recent years, data science agents powered by Large Language Models (LLMs), known as ``data agents,'' have shown significant potential to transform the traditional data analysis paradigm. This survey provides an overview of the evolution, capabilities, and applications of LLM-based data agents, highlighting their role in simplifying complex data tasks and lowering the entry barrier for users without related expertise. We explore current trends in the design of LLM-based frameworks, detailing essential features such as planning, reasoning, reflection, multi-agent collaboration, user interface, knowledge integration, and system design, which enable agents to address data-centric problems with minimal human intervention. Furthermore, we analyze several case studies to demonstrate the practical applications of various data agents in real-world scenarios. Finally, we identify key challenges and propose future research directions to advance the development of data agents into intelligent statistical analysis software.},
  file = {/Users/felixriedel/Zotero/storage/CHYZKB3K/Sun et al. - A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf}
}

@article{SUPRIYONO2024100173,
  title = {Advancements in Natural Language Processing: {{Implications}}, Challenges, and Future Directions},
  author = {{Supriyono} and Wibawa, Aji Prasetya and {Suyono} and Kurniawan, Fachrul},
  year = 2024,
  journal = {Telematics and Informatics Reports},
  volume = {16},
  pages = {100173},
  issn = {2772-5030},
  doi = {10.1016/j.teler.2024.100173},
  abstract = {This research delves into the latest advancements in Natural Language Processing (NLP) and their broader implications, challenges, and future directions. With the ever-increasing volume of text data generated daily from diverse sources, extracting relevant and valuable information is becoming more complex. Conventional manual techniques for handling and examining written information are laborious and susceptible to mistakes, underscoring the necessity for effective automated alternatives. The advancements in Natural Language Processing (NLP), namely in transformer-based models and deep learning techniques, have demonstrated considerable potential in improving the precision and consistency of various NLP applications. This work presents a novel approach that combines systematic review methods with sophisticated NLP approaches to enhance the overall efficiency of NLP systems. The proposed strategy guarantees an organized and clear literature review process, resulting in more informative and contextually relevant results. The report examines NLP's implications, problems, and opportunities, providing significant insights that are anticipated to propel improvements in NLP technology and its application in many industries.},
  keywords = {Deep learning techniques,Natural language processing,Systematic review methodologies,Text data analysis,Transformer models}
}

@article{VenkatSanka_2025,
  title = {Conversational {{AI}} for Enterprise Data Analytics and Governance: A Comprehensive Framework for Natural Language-Driven Business Intelligence},
  author = {Sanka, Venkat},
  year = 2025,
  month = apr,
  journal = {International Journal of Scientific Research in Computer Science, Engineering and Information Technology},
  volume = {11},
  number = {2},
  pages = {3922--3928},
  doi = {10.32628/CSEIT25111329},
  abstract = {The integration of conversational artificial intelligence into enterprise data analytics and governance represents a paradigm shift in how organizations interact with their data assets. This paper presents a comprehensive framework for implementing conversational AI systems that enable natural language querying, automated compliance monitoring, and intelligent data discovery in enterprise environments. The proposed architecture leverages advanced natural language processing techniques, including large language models and context-aware dialogue systems, to bridge the gap between business users and complex data infrastructures. Through empirical evaluation across three enterprise domains--- financial services, healthcare, and telecommunications---the framework demonstrates significant improvements in query response time (67}
}

@misc{WhatAreLarge2021,
  title = {What {{Are Large Language Models}} ({{LLMs}})? \textbar{} {{IBM}}},
  shorttitle = {What {{Are Large Language Models}} ({{LLMs}})?},
  year = 2021,
  month = oct,
  url = {https://www.ibm.com/think/topics/large-language-models},
  urldate = {2025-11-06},
  abstract = {Large language models are AI systems capable of understanding and generating human language by processing vast amounts of text data.},
  langid = {english},
  file = {/Users/felixriedel/Zotero/storage/E55SH7HC/large-language-models.html}
}

@misc{WhatHumanLoop2025,
  title = {What {{Is Human In The Loop}} ({{HITL}})? \textbar{} {{IBM}}},
  shorttitle = {What {{Is Human In The Loop}} ({{HITL}})?},
  year = 2025,
  month = jul,
  url = {https://www.ibm.com/think/topics/human-in-the-loop},
  urldate = {2025-10-29},
  abstract = {Human In The Loop (HITL) refers to a system or process in which a human actively participates in the operation, supervision or decision-making of an automated or AI-driven system.},
  langid = {english},
  file = {/Users/felixriedel/Zotero/storage/67X8UEIS/human-in-the-loop.html}
}

@unpublished{Wittpohl2025,
  title = {Conversational Analytics-App Architecture Ideation},
  author = {Wittpohl, Milan and Gieselmann, Thees},
  year = 2025,
  month = jul
}
